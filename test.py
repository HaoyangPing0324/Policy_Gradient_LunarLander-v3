"""
@Author  : å¹³æ˜Šé˜³
@Email   : pinghaoyang0324@163.com
@Time    : 2025/12/24
@Desc    : PGæ¨¡å‹æµ‹è¯•è„šæœ¬ï¼ˆåŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼ŒéªŒè¯LunarLander-v3æ€§èƒ½ï¼‰
@License : MIT License (MIT)
@Version : 1.0
"""

import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pygame.pkgdata")

import torch
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from policy_network import PolicyNetwork  # å¯¼å…¥ä½ çš„ç­–ç•¥ç½‘ç»œç±»

def load_policy_model(model_path, obs_dim=8, action_dim=4, hidden_layers=[256,128], activation='relu'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œæƒé‡
    :param model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ "./lunar_lander_policy_gradient.pth"ï¼‰
    :param obs_dim: çŠ¶æ€ç»´åº¦ï¼ˆå›ºå®š8ï¼‰
    :param action_dim: åŠ¨ä½œç»´åº¦ï¼ˆå›ºå®š4ï¼‰
    :param hidden_layers: éšè—å±‚é…ç½®ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
    :param activation: æ¿€æ´»å‡½æ•°ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
    :return: åŠ è½½å¥½æƒé‡çš„PolicyNetworkå®ä¾‹ï¼ˆevalæ¨¡å¼ï¼‰
    """
    # 1. ç¡®å®šè®¾å¤‡ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"æµ‹è¯•è®¾å¤‡ï¼š{device}")

    # 2. åˆ›å»ºç½‘ç»œå®ä¾‹ï¼ˆç»“æ„éœ€ä¸è®­ç»ƒå®Œå…¨åŒ¹é…ï¼‰
    policy_net = PolicyNetwork(
        obs_dim=obs_dim,
        action_dim=action_dim,
        hidden_layers=hidden_layers,
        activation=activation,
        device=device
    )

    # 3. åŠ è½½æƒé‡ï¼ˆå¤„ç†CPU/GPUå…¼å®¹ï¼‰
    try:
        # å…¼å®¹GPUè®­ç»ƒã€CPUæµ‹è¯•çš„åœºæ™¯
        state_dict = torch.load(model_path, map_location=device)
        policy_net.load_state_dict(state_dict)
        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼š{model_path}")
    except Exception as e:
        raise ValueError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}\nè¯·ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œä¸”ç½‘ç»œç»“æ„ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼")

    # 4. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨Dropout/BNç­‰è®­ç»ƒå±‚ï¼Œä¸å½±å“ä½ çš„MLPï¼Œä½†è§„èŒƒï¼‰
    policy_net.eval()
    return policy_net

def test_policy(policy_net, num_test_episodes=10, render=True):
    """
    æµ‹è¯•ç­–ç•¥ç½‘ç»œæ€§èƒ½
    :param policy_net: åŠ è½½å¥½çš„PolicyNetworkå®ä¾‹
    :param num_test_episodes: æµ‹è¯•å›åˆæ•°ï¼ˆæ¨èâ‰¥10ï¼Œç»Ÿè®¡æ›´ç¨³å®šï¼‰
    :param render: æ˜¯å¦æ¸²æŸ“ç”»é¢ï¼ˆTrue=å¯è§†åŒ–æµ‹è¯•ï¼ŒFalse=å¿«é€Ÿæµ‹è¯•ï¼‰
    :return: test_rewardsï¼ˆæ¯å›åˆå¾—åˆ†åˆ—è¡¨ï¼‰ã€avg_rewardï¼ˆå¹³å‡å¾—åˆ†ï¼‰
    """
    # åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒï¼ˆrender_modeæ§åˆ¶æ˜¯å¦æ˜¾ç¤ºç”»é¢ï¼‰
    render_mode = "human" if render else None
    env = gym.make("LunarLander-v3", render_mode=render_mode)

    test_rewards = []
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•ï¼ˆå…±{num_test_episodes}å›åˆï¼‰...")

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆæµ‹è¯•æ—¶æ— éœ€åå‘ä¼ æ’­ï¼Œæå‡é€Ÿåº¦ï¼‰
    with torch.no_grad():
        for episode in range(num_test_episodes):
            current_state, _ = env.reset()
            done = False
            total_reward = 0

            while not done:
                # æµ‹è¯•æ—¶ç”¨ã€Œæœ€ä¼˜åŠ¨ä½œã€ï¼ˆæ— éšæœºæ¢ç´¢ï¼‰ï¼Œä½“ç°ç­–ç•¥çœŸå®æ€§èƒ½
                action = policy_net.get_best_action(current_state)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated

                total_reward += reward
                current_state = next_state

                # æ¸²æŸ“æ—¶å¢åŠ å»¶è¿Ÿï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼ˆå¯é€‰ï¼‰
                if render:
                    import time
                    time.sleep(0.01)

            test_rewards.append(total_reward)
            print(f"æµ‹è¯•å›åˆ {episode+1:2d} | å¾—åˆ†ï¼š{total_reward:6.2f}")

    env.close()

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    avg_reward = np.mean(test_rewards)
    std_reward = np.std(test_rewards)  # å¾—åˆ†æ ‡å‡†å·®ï¼Œåæ˜ ç¨³å®šæ€§
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡ï¼š")
    print(f"å¹³å‡å¾—åˆ†ï¼š{avg_reward:6.2f} | å¾—åˆ†æ ‡å‡†å·®ï¼š{std_reward:5.2f}")
    print(f"æœ€é«˜å¾—åˆ†ï¼š{np.max(test_rewards):6.2f} | æœ€ä½å¾—åˆ†ï¼š{np.min(test_rewards):6.2f}")

    return test_rewards, avg_reward

def plot_test_results(test_rewards, avg_reward, save_path="test_reward_plot.png"):
    """
    ç»˜åˆ¶æµ‹è¯•å¾—åˆ†å¯è§†åŒ–å›¾è¡¨
    :param test_rewards: æ¯å›åˆå¾—åˆ†åˆ—è¡¨
    :param avg_reward: å¹³å‡å¾—åˆ†
    :param save_path: å›¾è¡¨ä¿å­˜è·¯å¾„
    """
    plt.figure(figsize=(10, 6))
    # é…ç½®ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

    # ç»˜åˆ¶æ¯å›åˆå¾—åˆ†
    x = range(1, len(test_rewards)+1)
    plt.plot(x, test_rewards, color="#1f77b4", linewidth=2, marker='o', label="å•å›åˆå¾—åˆ†")
    # ç»˜åˆ¶å¹³å‡å¾—åˆ†æ°´å¹³çº¿
    plt.axhline(y=avg_reward, color="#ff7f0e", linewidth=2, linestyle='--', label=f"å¹³å‡å¾—åˆ† ({avg_reward:.2f})")
    # ç»˜åˆ¶è¾¾æ ‡çº¿ï¼ˆ200åˆ†ï¼‰
    plt.axhline(y=200, color="#2ca02c", linewidth=1.5, linestyle=':', label="è¾¾æ ‡çº¿ï¼ˆ200åˆ†ï¼‰")

    # å›¾è¡¨æ ‡æ³¨
    plt.xlabel("æµ‹è¯•å›åˆæ•°", fontsize=12)
    plt.ylabel("å›åˆå¾—åˆ†", fontsize=12)
    plt.title("LunarLander-v3 ç­–ç•¥æµ‹è¯•å¾—åˆ†åˆ†å¸ƒ", fontsize=14, fontweight="bold")
    plt.legend(fontsize=10)
    plt.grid(alpha=0.3)
    plt.xticks(x)  # å¼ºåˆ¶æ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•å›åˆ

    # ä¿å­˜å›¾è¡¨
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    print(f"\nğŸ“¸ æµ‹è¯•å›¾è¡¨å·²ä¿å­˜ï¼š{save_path}")
    plt.show()

if __name__ == "__main__":
    # ===================== é…ç½®å‚æ•°ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰ =====================
    MODEL_PATH = "./lunar_lander_policy_gradient.pth"  # è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
    HIDDEN_LAYERS = [256, 128]  # éœ€ä¸è®­ç»ƒæ—¶çš„hidden_layerså®Œå…¨ä¸€è‡´
    ACTIVATION = 'relu'         # éœ€ä¸è®­ç»ƒæ—¶çš„æ¿€æ´»å‡½æ•°ä¸€è‡´
    NUM_TEST_EPISODES = 10      # æµ‹è¯•å›åˆæ•°ï¼ˆå»ºè®®10-20ï¼‰
    RENDER_TEST = True          # æ˜¯å¦å¯è§†åŒ–æµ‹è¯•ï¼ˆTrue=çœ‹ç”»é¢ï¼ŒFalse=å¿«é€Ÿè·‘ï¼‰

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ===================== æ‰§è¡Œæµ‹è¯•æµç¨‹ =====================
    # 1. åŠ è½½æ¨¡å‹
    policy_net = load_policy_model(
        model_path=MODEL_PATH,
        hidden_layers=HIDDEN_LAYERS,
        activation=ACTIVATION
    ).to(device)

    # 2. æµ‹è¯•ç­–ç•¥
    test_rewards, avg_reward = test_policy(
        policy_net=policy_net,
        num_test_episodes=NUM_TEST_EPISODES,
        render=RENDER_TEST
    )

    # 3. ç»˜åˆ¶æµ‹è¯•ç»“æœ
    plot_test_results(test_rewards, avg_reward)